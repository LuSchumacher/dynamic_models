{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from tqdm.notebook import tqdm\n",
    "# import talib\n",
    "import pickle\n",
    "import pystan\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cd/lsff0c7s4fn3mb649wt67m200000gn/T/ipykernel_30742/1308667103.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# gpu setting and checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# gpu setting and checking\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3200, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_gen(32, 3200)\n",
    "to_categorical(context_gen(32, 3200)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_prior(batch_size, fixed_var=None):\n",
    "    \"\"\"\n",
    "    Generates a random draw from the diffusion model prior.\n",
    "    \"\"\"\n",
    "    v = np.random.gamma(2.5, 1/1.5, (batch_size, 4))\n",
    "    a = np.random.gamma(4.0, 1/3.0, batch_size)\n",
    "    ndt = np.random.gamma(1.5, 1/5.0, batch_size)\n",
    "    theta_s = np.random.uniform(0.1, 0.1, (batch_size, 6))\n",
    "    if fixed_var != None:\n",
    "        theta_s = np.random.uniform(fixed_var, fixed_var, (batch_size, 6))\n",
    "\n",
    "    return np.c_[v, a, ndt, theta_s]\n",
    "\n",
    "\n",
    "@njit\n",
    "def context_gen(batch_size, n_obs):\n",
    "    \"\"\"\n",
    "    Generates an experimenmt wiht a random sequence of 4 conditions.\n",
    "    \"\"\"\n",
    "    obs_per_condition = int(n_obs / 4)\n",
    "    context = np.zeros((batch_size, n_obs), dtype=np.int32)\n",
    "    x = np.repeat([0, 1, 2, 3], obs_per_condition)\n",
    "    for i in range(batch_size):\n",
    "        np.random.shuffle(x)\n",
    "        context[i] = x\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "@njit\n",
    "def diffusion_trial(v, a, ndt, zr=0.5, dt=0.001, s=1.0, max_iter=1e4):\n",
    "    \"\"\"\n",
    "    Simulates a single reaction time from a simple drift-diffusion process.\n",
    "    \"\"\"\n",
    "\n",
    "    n_iter = 0\n",
    "    x = a * zr\n",
    "    c = np.sqrt(dt * s)\n",
    "    \n",
    "    while x > 0 and x < a:\n",
    "        \n",
    "        # DDM equation\n",
    "        x += v*dt + c * np.random.randn()\n",
    "        \n",
    "        n_iter += 1\n",
    "        \n",
    "    rt = n_iter * dt\n",
    "    return rt+ndt if x >= 0 else -(rt+ndt)\n",
    "\n",
    "\n",
    "@njit\n",
    "def dynamic_diffusion_process(prior_samples, context, n_obs):\n",
    "    \"\"\"\n",
    "    Performs one run of a dynamic diffusion model process.\n",
    "    \"\"\"\n",
    "    params_t, theta_s = np.split(prior_samples, 2, axis=-1)\n",
    "    theta_d = np.zeros((n_obs, params_t.shape[0]))\n",
    "    \n",
    "    # Draw first param combination from prior\n",
    "    rt = np.zeros(n_obs)\n",
    "    \n",
    "    # Iterate over number of trials\n",
    "    for t in range(n_obs):\n",
    "        \n",
    "        # Run diffusion process\n",
    "        rt[t] = diffusion_trial(params_t[context[t]], params_t[4], params_t[5])\n",
    "        \n",
    "        # Store before transition\n",
    "        theta_d[t] = params_t\n",
    "        \n",
    "        # Transition and ensure non-negative parameters\n",
    "        params_t = params_t + theta_s * np.random.randn(params_t.shape[0])\n",
    "        \n",
    "        # Constraints\n",
    "        params_t[0] = min(max(params_t[0], 0.0), 8)\n",
    "        params_t[1] = min(max(params_t[1], 0.0), 8)\n",
    "        params_t[2] = min(max(params_t[2], 0.0), 8)\n",
    "        params_t[3] = min(max(params_t[3], 0.0), 8)\n",
    "        params_t[4] = min(max(params_t[4], 0.0), 6)\n",
    "        params_t[5] = min(max(params_t[5], 0.0), 4)\n",
    "        \n",
    "    return np.atleast_2d(rt).T, theta_d, theta_s\n",
    "\n",
    "\n",
    "@njit\n",
    "def dynamic_batch_simulator(prior_samples, context):\n",
    "    \"\"\"\n",
    "    Performs one batch of dynamic diffusion model runs.\n",
    "    \"\"\"\n",
    "    batch_size = prior_samples.shape[0]\n",
    "    n_obs = context.shape[1]\n",
    "    rt = np.zeros((batch_size, n_obs, 1))\n",
    "    theta_d = np.zeros((batch_size, n_obs, 6))\n",
    "    theta_s = np.zeros((batch_size, 6))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        rt[i], theta_d[i], theta_s[i] = dynamic_diffusion_process(prior_samples[i], \n",
    "                                                                  context[i],\n",
    "                                                                  n_obs)\n",
    "    \n",
    "    return np.concatenate((rt, np.expand_dims(context, axis=2)), axis=-1), theta_d, theta_s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def static_diffusion_process(prior_samples, context, n_obs):\n",
    "    \"\"\"\n",
    "    Performs one run of a static diffusion model process.\n",
    "    \"\"\"\n",
    "    \n",
    "    params_t, params_stds = np.split(prior_samples, 2, axis=-1)\n",
    "    \n",
    "    rt = np.zeros(n_obs)\n",
    "    \n",
    "    # Iterate over number of trials\n",
    "    for t in range(n_obs):\n",
    "        \n",
    "        # Run diffusion process\n",
    "        rt[t] = diffusion_trial(params_t[context[t]], params_t[4], params_t[5])\n",
    "        \n",
    "    return np.atleast_2d(rt).T, params_t\n",
    "\n",
    "\n",
    "@njit\n",
    "def static_batch_simulator(prior_samples, context):\n",
    "    \"\"\"\n",
    "    Performs one batch of static diffusion model runs.\n",
    "    \"\"\"\n",
    "    batch_size = prior_samples.shape[0]\n",
    "    n_obs = context.shape[1]\n",
    "    rt = np.zeros((batch_size, n_obs, 1))\n",
    "    theta = np.zeros((batch_size, n_obs, 6))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rt[i], theta[i] = static_diffusion_process(prior_samples[i], \n",
    "                                                   context[i],\n",
    "                                                   n_obs)\n",
    "    return np.concatenate((rt, np.expand_dims(context, axis=2)), axis=-1), theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N;                 \n",
    "  real<lower=0> rt[N];    \n",
    "  int<lower=0,upper=1> correct[N];\n",
    "  int<lower=1,upper=4> context[N];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> v[4];\n",
    "  real<lower=0> a; \n",
    "  real<lower=0> ndt;\n",
    "}\n",
    "\n",
    "model {\n",
    "  // Priors\n",
    "  v ~ gamma(2.5, 1.5);\n",
    "  a ~ gamma(4.0, 3.0);\n",
    "  ndt ~ gamma(1.5, 5.0);\n",
    "  \n",
    "  for (n in 1:N) {\n",
    "     if (correct[n] == 1) {\n",
    "        rt[n] ~ wiener(a, ndt, 0.5, v[context[n]]);\n",
    "     } \n",
    "     else {\n",
    "        rt[n] ~ wiener(a, ndt, 1 - 0.5, -v[context[n]]);\n",
    "     }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_stan(sim_data):\n",
    "    \"\"\"\n",
    "    Convert data from simulator to stan-friendly format.\n",
    "    \"\"\"\n",
    "    rt = sim_data[:, 0]\n",
    "    context = sim_data[:, 1].astype(np.int32)\n",
    "    correct = (rt >= 0).astype(np.int32)\n",
    "    rt = np.abs(rt).astype(np.float32)\n",
    "    return {'rt': rt, 'correct': correct, 'context': context, 'N': rt.shape[0]}\n",
    "\n",
    "def loop_stan(data, verbose=True):\n",
    "    \"\"\"\n",
    "    Loop through data and obtain posteriors.\n",
    "    \"\"\"\n",
    "    \n",
    "    stan_post_samples = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_i = to_stan(data[i])\n",
    "        ndt_init = data_i['rt'].min() * 0.75\n",
    "        init = {'ndt': ndt_init}\n",
    "        fit = sm.sampling(data=data_i, \n",
    "                          iter=2000, chains=4, n_jobs=4, init=[init, init, init, init],\n",
    "                          control=dict(adapt_delta=0.99, max_treedepth=15))\n",
    "        samples = fit.extract(permuted=True)\n",
    "        stan_post_samples.append(samples)\n",
    "        if verbose:\n",
    "            print(f'Finished estimating data set {i + 1}...')\n",
    "    return stan_post_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS = 800\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelization for stan\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"fork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile stan model\n",
    "sm = pystan.StanModel(model_code=stan_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No variance (static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 800, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_draws = dynamic_prior(BATCH_SIZE)\n",
    "context = context_gen(BATCH_SIZE, N_OBS)\n",
    "sim_data, theta_d = static_batch_simulator(prior_draws, context)\n",
    "sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "which_param = 0\n",
    "plt.plot(np.arange(N_OBS), theta_d[0, :, which_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_post_samples = loop_stan(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "{\n",
    "    'rt': sim_data[:, : , 0], \n",
    "    'context': sim_data[:, : , 1], \n",
    "    'theta_d': theta_d,\n",
    "    'stan_post_samples': stan_post_samples\n",
    "    }, \n",
    "    open('./simulation_800_noVar.pkl', 'wb+')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_draws = dynamic_prior(BATCH_SIZE, fixed_var=0.001)\n",
    "context = context_gen(BATCH_SIZE, N_OBS)\n",
    "sim_data, theta_d, theta_s = dynamic_batch_simulator(prior_draws, context)\n",
    "sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "which_param = 0\n",
    "plt.plot(np.arange(N_OBS), theta_d[0, :, which_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_post_samples = loop_stan(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "{\n",
    "    'rt': sim_data[:, : , 0], \n",
    "    'context': sim_data[:, : , 1], \n",
    "    'theta_d': theta_d,\n",
    "    'theta_s': theta_s,\n",
    "    'stan_post_samples': stan_post_samples\n",
    "    }, \n",
    "    open('./sim_800_lowVar.pkl', 'wb+')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 800, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_draws = dynamic_prior(BATCH_SIZE, fixed_var=0.1)\n",
    "context = context_gen(BATCH_SIZE, N_OBS)\n",
    "sim_data, theta_d, theta_s = dynamic_batch_simulator(prior_draws, context)\n",
    "sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "which_param = 0\n",
    "plt.plot(np.arange(N_OBS), theta_d[0, :, which_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_post_samples = loop_stan(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "{\n",
    "    'rt': sim_data[:, : , 0], \n",
    "    'context': sim_data[:, : , 1], \n",
    "    'theta_d': theta_d,\n",
    "    'theta_s': theta_s,\n",
    "    'stan_post_samples': stan_post_samples\n",
    "    }, \n",
    "    open('./sim_800_highVar.pkl', 'wb+')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amortizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroscedasticNetwork(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_params_d):\n",
    "        super(HeteroscedasticNetwork, self).__init__()\n",
    "        \n",
    "        self.preprocessor = Sequential([\n",
    "            GRU(64, return_sequences=True),\n",
    "            LSTM(128, return_sequences=True),\n",
    "            Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "        ])\n",
    "        \n",
    "        self.dynamic_predictor = Sequential([\n",
    "            Dense(64, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            tf.keras.layers.Dense(tfpl.MultivariateNormalTriL.params_size(n_params_d)),\n",
    "            tfpl.MultivariateNormalTriL(n_params_d)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain representation\n",
    "        rep = self.preprocessor(x)\n",
    "        \n",
    "        # Predict dynamic stuff\n",
    "        preds_dyn = self.dynamic_predictor(rep)\n",
    "     \n",
    "        return preds_dyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_epoch_trainer(network, optimizer, batch_size, n_obs, steps_per_epoch, p_bar):\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            # Simulate from model\n",
    "            prior_draws = dynamic_prior(batch_size)\n",
    "            context = context_gen(batch_size, n_obs)\n",
    "            sim_data, theta_d = static_batch_simulator(prior_draws, context)\n",
    "\n",
    "            net_in = tf.concat((sim_data[:, :, :1], to_categorical(sim_data[:, :, 1:])), axis=-1)\n",
    "            pred_theta_d = network(net_in)\n",
    "\n",
    "            # Loss dynamic and static\n",
    "            loss_d = nll(theta_d, pred_theta_d)\n",
    "            \n",
    "            total_loss = loss_d\n",
    "        g = tape.gradient(total_loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(total_loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss D: {:.3f} Running Loss: {:.3f}\"\n",
    "                              .format(ep, step, loss_d.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n",
    "\n",
    "\n",
    "def dynamic_epoch_trainer(network, optimizer, batch_size, n_obs, steps_per_epoch, p_bar, fixed_var):\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            # Simulate from model\n",
    "            prior_draws = dynamic_prior(batch_size, fixed_var)\n",
    "            context = context_gen(batch_size, n_obs)\n",
    "            x_t, theta_d, _= dynamic_batch_simulator(prior_draws, context)\n",
    "\n",
    "            net_in = tf.concat((sim_data[:, :, :1], sim_data[:, :, 1:]), axis=-1)\n",
    "            pred_theta_d = network(net_in)\n",
    "\n",
    "            # Loss dynamic and static\n",
    "            loss_d = nll(theta_d, pred_theta_d)\n",
    "            \n",
    "            total_loss = loss_d\n",
    "        g = tape.gradient(total_loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(total_loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss D: {:.3f} Running Loss: {:.3f}\"\n",
    "                              .format(ep, step, loss_d.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS = 800\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "STEPS_PER_EPOCH = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No variance (static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "network = HeteroscedasticNetwork(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(1, EPOCHS+1):\n",
    "    with tqdm(total=STEPS_PER_EPOCH, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "        static_epoch_trainer(network, optimizer, BATCH_SIZE, N_OBS, STEPS_PER_EPOCH, p_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save_weights('checkpoints/static_800')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "network = HeteroscedasticNetwork(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(1, EPOCHS+1):\n",
    "    with tqdm(total=STEPS_PER_EPOCH, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "        dynamic_epoch_trainer(network, optimizer, BATCH_SIZE, N_OBS, STEPS_PER_EPOCH, p_bar, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save_weights('checkpoints/lowVar_800')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "network = HeteroscedasticNetwork(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(1, EPOCHS+1):\n",
    "    with tqdm(total=STEPS_PER_EPOCH, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "        dynamic_epoch_trainer(network, optimizer, BATCH_SIZE, N_OBS, STEPS_PER_EPOCH, p_bar, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save_weights('checkpoints/lowVar_800')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5c1a6784774cb00c4ca0a99fd750e2e4caf18aa135f6d49553516d8651234b4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('cognitiveModeling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
