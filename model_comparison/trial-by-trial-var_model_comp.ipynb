{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from tqdm.notebook import tqdm\n",
    "import talib\n",
    "\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../')))\n",
    "from generative_models import *\n",
    "from fast_dm_simulator import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu setting and checking\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amortizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroscedasticNetwork(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_params_d, n_params_s):\n",
    "        super(HeteroscedasticNetwork, self).__init__()\n",
    "        \n",
    "        self.preprocessor = Sequential([\n",
    "            GRU(64, return_sequences=True),\n",
    "            LSTM(128, return_sequences=True),\n",
    "            Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "        ])\n",
    "        \n",
    "        self.dynamic_predictor = Sequential([\n",
    "            Dense(64, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            tf.keras.layers.Dense(tfpl.MultivariateNormalTriL.params_size(n_params_d)),\n",
    "            tfpl.MultivariateNormalTriL(n_params_d)\n",
    "        ])\n",
    "\n",
    "        self.static_predictor = Sequential([\n",
    "            LSTM(n_params_s),\n",
    "            Dense(tfpl.MultivariateNormalTriL.params_size(n_params_s)),\n",
    "            tfpl.MultivariateNormalTriL(n_params_s)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain representation\n",
    "        rep = self.preprocessor(x)\n",
    "        \n",
    "        # Predict dynamic\n",
    "        preds_dyn = self.dynamic_predictor(rep)\n",
    "\n",
    "        # predict static\n",
    "        preds_stat = self.static_predictor(rep)\n",
    "\n",
    "        return preds_dyn, preds_stat\n",
    "\n",
    "class StaticHeteroscedasticNetwork(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_params_d):\n",
    "        super(HeteroscedasticNetwork, self).__init__()\n",
    "        \n",
    "        self.preprocessor = Sequential([\n",
    "            GRU(64, return_sequences=True),\n",
    "            LSTM(128, return_sequences=True),\n",
    "            Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "        ])\n",
    "        \n",
    "        self.dynamic_predictor = Sequential([\n",
    "            Dense(64, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            tf.keras.layers.Dense(tfpl.MultivariateNormalTriL.params_size(n_params_d)),\n",
    "            tfpl.MultivariateNormalTriL(n_params_d)\n",
    "        ])\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \"\"\"\n",
    "        # Obtain representation\n",
    "        rep = self.preprocessor(x)\n",
    "        \n",
    "        # Predict dynamic\n",
    "        preds_dyn = self.dynamic_predictor(rep)\n",
    "\n",
    "        return preds_dyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_trainer(network, optimizer, batch_size, n_obs, steps_per_epoch, p_bar):\n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Simulate from model\n",
    "            prior_draws = dynamic_prior(batch_size)\n",
    "            context = context_gen(batch_size, n_obs)\n",
    "            sim_data, theta_d, theta_s = dynamic_batch_simulator(prior_draws, context)\n",
    "\n",
    "            # predict\n",
    "            net_in = tf.concat((sim_data[:, :, :1], to_categorical(sim_data[:, :, 1:])), axis=-1)\n",
    "            pred_theta_d, pred_theta_s = network(net_in)\n",
    "\n",
    "            # loss\n",
    "            loss_d = nll(theta_d, pred_theta_d)\n",
    "            loss_s = nll(theta_s, pred_theta_s)\n",
    "            total_loss = loss_d + loss_s\n",
    "\n",
    "        g = tape.gradient(total_loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(total_loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss D: {:.3f} Running Loss: {:.3f}\"\n",
    "                              .format(ep, step, loss_d.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n",
    "\n",
    "def static_epoch_trainer(network, optimizer, batch_size, n_obs, steps_per_epoch, p_bar):\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            # Simulate from model\n",
    "            prior_draws = dynamic_prior(batch_size)\n",
    "            context = context_gen(batch_size, n_obs)\n",
    "            sim_data, theta_d = static_batch_simulator(prior_draws, context)\n",
    "\n",
    "            net_in = tf.concat((sim_data[:, :, :1], to_categorical(sim_data[:, :, 1:])), axis=-1)\n",
    "            pred_theta_d = network(net_in)\n",
    "\n",
    "            # Loss \n",
    "            loss_d = nll(theta_d, pred_theta_d)\n",
    "            \n",
    "            total_loss = loss_d\n",
    "        g = tape.gradient(total_loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(total_loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss D: {:.3f} Running Loss: {:.3f}\"\n",
    "                              .format(ep, step, loss_d.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS = 3200\n",
    "BATCH_SIZE = 8\n",
    "STEPS_PER_EPOCH = 1000\n",
    "epochs = [50, 25, 25]\n",
    "learning_rates = [0.0001, 0.00005, 0.00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize net\n",
    "network = HeteroscedasticNetwork(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(epochs)):\n",
    "    # set learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rates[i])\n",
    "    \n",
    "    # train epochs\n",
    "    for ep in range(1, epochs[i]+1):\n",
    "        with tqdm(total=STEPS_PER_EPOCH, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "            epoch_trainer(network, optimizer, BATCH_SIZE, N_OBS, STEPS_PER_EPOCH, p_bar)\n",
    "    \n",
    "    #save weights\n",
    "    network.save_weights('checkpoints/varying_hyperparams_3200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize net\n",
    "network = StaticHeteroscedasticNetwork(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(epochs)):\n",
    "    # set learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rates[i])\n",
    "    \n",
    "    # train epochs\n",
    "    for ep in range(1, epochs[i]+1):\n",
    "        with tqdm(total=STEPS_PER_EPOCH, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "            static_epoch_trainer(network, optimizer, BATCH_SIZE, N_OBS, STEPS_PER_EPOCH, p_bar)\n",
    "    \n",
    "    #save weights\n",
    "    network.save_weights('checkpoints/static_ddm_3200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize net\n",
    "network = HeteroscedasticNetwork(6, 6)\n",
    "network.load_weights('checkpoints/varying_hyperparams_3200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv('../../data/data_lexical_decision.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data\n",
    "which = 5\n",
    "person_data = data[data.id == which]\n",
    "person_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for fitting\n",
    "# negative rts for error responses\n",
    "person_data.rt[person_data.acc == 0] = -person_data.rt[person_data.acc == 0]\n",
    "rt = np.array([person_data.rt])[:, :, np.newaxis]\n",
    "stim_type = np.array([person_data.stim_type])[:, :, np.newaxis] - 1 \n",
    "context = to_categorical(stim_type)\n",
    "x_nn = tf.concat((rt, context), axis=-1)\n",
    "x_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amortized inference\n",
    "post_d, post_s = network(x_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set font type\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.serif'] = \"Palatino\"\n",
    "matplotlib.rcParams['font.family'] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Returns tuple of handles, labels for axis ax, after reordering them to conform to the label order `order`, and if unique is True, after removing entries with duplicate labels.\n",
    "def reorderLegend(f=None,order=None,unique=False):\n",
    "    if f is None: f=plt.gca()\n",
    "    handles, labels = f.get_legend_handles_labels()\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0])) # sort both labels and handles by labels\n",
    "    if order is not None: # Sort according to a given list (not necessarily complete)\n",
    "        keys=dict(zip(order,range(len(order))))\n",
    "        labels, handles = zip(*sorted(zip(labels, handles), key=lambda t,keys=keys: keys.get(t[0],np.inf)))\n",
    "    if unique:  labels, handles= zip(*unique_everseen(zip(labels,handles), key = labels)) # Keep only the first of each handle\n",
    "    f.legend(handles, labels,\n",
    "              fontsize=16, loc='center', \n",
    "              bbox_to_anchor=(1.9, -0.4),fancybox=False, shadow=False, ncol=4)\n",
    "    return(handles, labels)\n",
    "\n",
    "\n",
    "def unique_everseen(seq, key=None):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x,k in zip(seq,key) if not (k in seen or seen_add(k))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamic_posteriors(dynamic_posterior, fast_dm_params, par_labels, par_names, \n",
    "                            ground_truths=None, color_pred='#852626'):\n",
    "    \"\"\"\n",
    "    Inspects the dynamic posterior given a single data set. Assumes six dynamic paramters.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(dynamic_posterior.shape) == 3, \"Dynamic posterior should be 3-dimensional!\" \n",
    "    assert ground_truths is None or len(ground_truths.shape) == 2,'Ground truths should be 2-dimensional!'\n",
    "    assert dynamic_posterior.shape[0] == 1, \"Function assumes dynamics posterior for a single data set!\" \n",
    "        \n",
    "    means = dynamic_posterior.mean()[0]\n",
    "    std = dynamic_posterior.stddev()[0]\n",
    "    \n",
    "    sigma_factors = [1]\n",
    "    alphas = [0.6]\n",
    "    serif_font = \"Computer Modern Roman\"\n",
    "    serif_font = \"Palatino\"\n",
    "\n",
    "    time = np.arange(x_nn.shape[1])\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    counter = 0\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        \n",
    "        ax.plot(time, means[:, i], color=color_pred, label='Posterior mean')\n",
    "        for sigma_factor, alpha in zip(sigma_factors, alphas):\n",
    "            ci_upper = means[:, i] + sigma_factor * std[:, i]\n",
    "            ci_lower = means[:, i] - sigma_factor * std[:, i]\n",
    "            ax.fill_between(time, ci_upper, ci_lower, color=color_pred, alpha=alpha, linewidth=0, label='Posterior sd')\n",
    "        if ground_truths is not None:\n",
    "            ax.plot(time, ground_truths[:, i], color='black', linestyle='dashed', label='True Dynamic', lw=2)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.set_xlabel('Time (t)', fontsize=18)\n",
    "        ax.set_ylabel('Parameter value ({})'.format(par_names[i]), fontsize=18)\n",
    "        ax.set_title(par_labels[i] + ' ({})'.format(par_names[i]), fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "        ax.grid(False)\n",
    "\n",
    "        # vertical bars\n",
    "        for idx in np.arange(799, 2400, 800):\n",
    "            if idx == 799:\n",
    "                ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "            else:\n",
    "                ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "        for idx in np.arange(99, 3100, 100):\n",
    "            if idx == 99:\n",
    "                ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "            else:\n",
    "                ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "\n",
    "        \n",
    "\n",
    "        # horizontal fast-dm params\n",
    "        if i <= 3:\n",
    "            ax.plot(time, np.repeat(params_fast_dm[i], x_nn.shape[1]), color='#598f70', alpha=1, label='Fast-dm estimate')\n",
    "            ax.fill_between(time, params_fast_dm[i] - params_fast_dm[6], params_fast_dm[i] + params_fast_dm[6], color='#598f70', alpha=0.3, linewidth=0, label='Fast-dm variability')\n",
    "        elif i == 4:\n",
    "            ax.plot(time, np.repeat(params_fast_dm[i], x_nn.shape[1]), color='#598f70', alpha=1, label='Fast-dm estimate')\n",
    "        else:\n",
    "            ax.plot(time, np.repeat(params_fast_dm[i], x_nn.shape[1]), color='#598f70', alpha=1, label='Fast-dm estimate')\n",
    "            ax.fill_between(time, params_fast_dm[i] - params_fast_dm[7]/2, params_fast_dm[i] + params_fast_dm[7]/2, color='#598f70', alpha=0.3, linewidth=0, label='Fast-dm variability')\n",
    "\n",
    "        # Shrink current axis's height by 10% on the bottom\n",
    "        # box = ax.get_position()\n",
    "        # ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "        #                 box.width, box.height * 0.9])\n",
    "\n",
    "        f.subplots_adjust(hspace=0.5)\n",
    "        if i == 0:\n",
    "            f.legend(fontsize=16, loc='center', \n",
    "                     bbox_to_anchor=(0.5, -0.05),fancybox=False, shadow=False, ncol=4)\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"plots/param_dynamic.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter dynamics: Empiric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fast-dm parameter estimates\n",
    "fast_dm_params = pd.read_csv('parameters_full_ddm_error_coding_cs.lst', encoding='iso-8859-1', header=0, delim_whitespace=True)\n",
    "fast_dm_params['dataset'] = fast_dm_params['dataset'].str.extract('(\\d+)').astype(int)\n",
    "fast_dm_params = fast_dm_params[['dataset', 'v_1', 'v_2', 'v_3', 'v_4', 'a', 't0', 'sv', 'st0']]\n",
    "fast_dm_params = fast_dm_params.sort_values('dataset')\n",
    "fast_dm_params = fast_dm_params.reset_index(drop=True)\n",
    "\n",
    "# subset fast_dm_params\n",
    "params_fast_dm = fast_dm_params[fast_dm_params.dataset == which].values[0, 1:]\n",
    "context = np.array(person_data['stim_type'] - 1)\n",
    "pred_rt_fast_dm = fast_dm_simulate(params_fast_dm, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_labels = ['Drift rate 1', 'Drift rate 2', 'Drift rate 3', 'Drift rate 4', 'Threshold', 'Non-decision time']\n",
    "par_names = [r'$v_1$', r'$v_2$', r'$v_3$', r'$v_4$', r'$a$', r'$\\tau$']\n",
    "plot_dynamic_posteriors(post_d, params_fast_dm, par_labels, par_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior retrodictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_check(emp_data, post_d, n_sim, sma_period=5):\n",
    "    # get experimental context\n",
    "    context = emp_data.stim_type.values - 1\n",
    "    # get empirical response times\n",
    "    emp_rt = np.abs(emp_data.rt.values)\n",
    "    sma_emp_rt = talib.SMA(emp_rt, timeperiod=sma_period)\n",
    "    \n",
    "    # sample from posterior\n",
    "    theta_d = np.array([post_d.sample(n_sim)])[0, :, 0, :]\n",
    "\n",
    "    n_obs = emp_rt.shape[0]\n",
    "    pred_rt = np.zeros((n_sim, n_obs))\n",
    "    sma_pred_rt = np.zeros((n_sim, n_obs))\n",
    "    # iterate over number of simulations\n",
    "    for sim in range(n_sim):\n",
    "        # Iterate over number of trials\n",
    "        rt = np.zeros(n_obs)\n",
    "        for t in range(n_obs):\n",
    "            # Run diffusion process\n",
    "            rt[t] = diffusion_trial(theta_d[sim, t, context[t]], theta_d[sim, t, 4], theta_d[sim, t, 5])\n",
    "        pred_rt[sim] = np.abs(rt)\n",
    "        sma_pred_rt[sim] = talib.SMA(np.abs(rt), timeperiod=sma_period)\n",
    "\n",
    "    return pred_rt, sma_pred_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred_rt_dynamic, sma_pred_rt_dynamic = pr_check(person_data, post_d, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize predicted response times\n",
    "quantiles = np.quantile(sma_pred_rt_dynamic, [0.05, 0.95], axis=0)\n",
    "median = np.median(sma_pred_rt_dynamic, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare emp rt data\n",
    "emp_rt = np.abs(person_data.rt.values)\n",
    "n_obs = emp_rt.shape[0]\n",
    "sma_emp_rt = talib.SMA(emp_rt, timeperiod=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fast-dm parameter estimates\n",
    "fast_dm_params = pd.read_csv('parameters_full_ddm_error_coding_cs.lst', encoding='iso-8859-1', header=0, delim_whitespace=True)\n",
    "fast_dm_params['dataset'] = fast_dm_params['dataset'].str.extract('(\\d+)').astype(int)\n",
    "fast_dm_params = fast_dm_params[['dataset', 'v_1', 'v_2', 'v_3', 'v_4', 'a', 't0', 'sv', 'st0']]\n",
    "fast_dm_params = fast_dm_params.sort_values('dataset')\n",
    "fast_dm_params = fast_dm_params.reset_index(drop=True)\n",
    "\n",
    "# subset fast_dm_params\n",
    "params_fast_dm = fast_dm_params[fast_dm_params.dataset == which].values[0, 1:]\n",
    "context = np.array(person_data['stim_type'] - 1)\n",
    "pred_rt_fast_dm = fast_dm_simulate(params_fast_dm, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorderLegend2(ax=None,order=None,unique=False):\n",
    "    if ax is None: ax=plt.gca()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0])) # sort both labels and handles by labels\n",
    "    if order is not None: # Sort according to a given list (not necessarily complete)\n",
    "        keys=dict(zip(order,range(len(order))))\n",
    "        labels, handles = zip(*sorted(zip(labels, handles), key=lambda t,keys=keys: keys.get(t[0],np.inf)))\n",
    "    if unique:  labels, handles= zip(*unique_everseen(zip(labels,handles), key = labels)) # Keep only the first of each handle\n",
    "    ax.legend(handles, labels,\n",
    "              fontsize=16, loc='upper right')\n",
    "    return(handles, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "f, ax = plt.subplots(1, 2, figsize=(18, 8),\n",
    "                     gridspec_kw={'width_ratios': [6, 1]})\n",
    "\n",
    "axrr = ax.flat\n",
    "\n",
    "# plot empiric and predicted response times series\n",
    "time = np.arange(n_obs) \n",
    "axrr[0].plot(time, sma_emp_rt, color='black', lw=1.5, label='SMA5: Empiric')\n",
    "axrr[0].plot(time, median, color='#598f70', lw=1.5, label='SMA5: Predicted median', alpha=0.8)\n",
    "axrr[0].fill_between(time, quantiles[0, :], quantiles[1, :], color='#598f70', linewidth=0, alpha=0.5, label='Predictive uncertainty')\n",
    "for idx in np.argwhere(person_data.session.diff().values == 1):\n",
    "    if idx == 800:\n",
    "        axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "    else:\n",
    "        axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "for idx in np.argwhere(person_data.block.diff().values == 1):\n",
    "    if idx == 100:\n",
    "        axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "    else:\n",
    "        axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "sns.despine(ax=axrr[0])\n",
    "axrr[0].grid(alpha=0.3)\n",
    "axrr[0].set_ylabel('RT(s)', fontsize=18)\n",
    "axrr[0].set_xlabel('Time(t)', fontsize=18)\n",
    "axrr[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "reorderLegend2(axrr[0],['SMA5: Empiric', 'SMA5: Predicted median', 'Predictive uncertainty'])\n",
    "axrr[0].grid(b=None)\n",
    "\n",
    "# plot empiric and predicted response time dist\n",
    "# axrr[1].get_shared_y_axes().join(axrr[1], axrr[0])\n",
    "# axrr[0].sharey(axrr[1])\n",
    "plt.setp(ax, ylim=(0, 1.5))\n",
    "sns.kdeplot(y=np.abs(emp_rt), fill=\"black\", color=\"black\", linewidth=1.5, alpha=0.3, label=\"Empiric\", ax=axrr[1])\n",
    "sns.kdeplot(y=np.abs(pred_rt_fast_dm), fill= '#852626', color=\"#852626\", alpha=0.3, linewidth=1.5, label=\"Fast-dm\", ax=axrr[1])\n",
    "sns.kdeplot(y=pred_rt_dynamic.flatten(),fill=\"#598f70\", color=\"#598f70\", linewidth=1.5, alpha=0.5, label=\"Dynamic dm\", ax=axrr[1])\n",
    "\n",
    "axrr[1].legend(fontsize=16)\n",
    "axrr[1].set_xlabel('Density', fontsize=18)\n",
    "axrr[1].tick_params(axis='both', which='major', labelsize=16)\n",
    "axrr[1].set_yticklabels('')\n",
    "sns.despine(ax=axrr[1])\n",
    "plt.subplots_adjust(wspace = 0.05)\n",
    "f.tight_layout()\n",
    "f.savefig(\"plots/rt_time_series.png\", dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empiric vs. predicted rt distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(pred_rt.flatten(), fill=\"navy\", color=\"navy\", label=\"Predicted\")\n",
    "sns.kdeplot(np.random.choice(np.abs(person_data.rt.values), size=pred_rt.flatten().size, replace=True), fill=\"maroon\", color=\"maroon\", label=\"Empiric\")\n",
    "sns.despine()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter dynamics: Simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic simulation\n",
    "n_obs = 800\n",
    "n_test = 6\n",
    "prior_draws = dynamic_prior(n_test)\n",
    "context = context_gen(n_test, n_obs)\n",
    "sim_data, theta_d, theta_s = dynamic_batch_simulator(prior_draws, context)\n",
    "context = to_categorical(sim_data[:, :, 1])\n",
    "rt = sim_data[:, :, 0, np.newaxis]\n",
    "x_nn = tf.concat((rt, context), axis=-1)\n",
    "x_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = 4\n",
    "post_d, post_s = network(x_nn[which:(which+1)])\n",
    "ground_truths = theta_d[which]\n",
    "par_labels = ['Drift rate 1', 'Drift rate 2', 'Drift rate 3', 'Drift rate 4', 'Threshold', 'Non-decision time']\n",
    "par_names = [r'$v_1$', r'$v_2$', r'$v_3$', r'$v_3$', r'$a$', r'$\\tau$']\n",
    "plot_dynamic_posteriors(post_d, par_labels, par_names, ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Fast-dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read parameter estimates\n",
    "fast_dm_params = pd.read_csv('parameters_full_ddm_error_coding_cs.lst', encoding='iso-8859-1', header=0, delim_whitespace=True)\n",
    "fast_dm_params['dataset'] = fast_dm_params['dataset'].str.extract('(\\d+)').astype(int)\n",
    "fast_dm_params = fast_dm_params[['dataset', 'v_1', 'v_2', 'v_3', 'v_4', 'a', 't0', 'sv', 'st0']]\n",
    "fast_dm_params = fast_dm_params.sort_values('dataset')\n",
    "fast_dm_params = fast_dm_params.reset_index(drop=True)\n",
    "fast_dm_params = fast_dm_params.to_numpy()\n",
    "fast_dm_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/data_lexical_decision.csv', sep=',', header=0)\n",
    "person_data = data[data.id == 2]\n",
    "\n",
    "params_fast_dm = fast_dm_params[1, 1:]\n",
    "context = np.array(person_data['stim_type'] - 1)\n",
    "# pred_rt_fast_dm = fast_dm_simulate(params_fast_dm, context)\n",
    "n_sub = len(np.unique(data.id))\n",
    "pred_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in np.unique(data.id):\n",
    "    # subset emp_data\n",
    "    person_data = data[data.id == sub]\n",
    "    n_obs = person_data.shape[0]\n",
    "    person_data.rt[person_data.acc == 0] = -person_data.rt[person_data.acc == 0]\n",
    "    emp_rt = person_data.rt\n",
    "\n",
    "    # subset fast_dm_params\n",
    "    params_fast_dm = fast_dm_params[sub-1, 1:]\n",
    "    context = np.array(person_data['stim_type'] - 1)\n",
    "    pred_rt_fast_dm = fast_dm_simulate(params_fast_dm, context)\n",
    "\n",
    "    # get dynamic_posteriors\n",
    "    rt = np.array([person_data.rt])[:, :, np.newaxis]\n",
    "    stim_type = np.array([person_data.stim_type])[:, :, np.newaxis] - 1 \n",
    "    context_estm = to_categorical(stim_type)\n",
    "    x_nn = tf.concat((rt, context_estm), axis=-1)\n",
    "    post_d, post_s = network(x_nn)\n",
    "\n",
    "    # predict rts with dynamic model\n",
    "    dynamic_posterior_mean = np.array(post_d.mean()[0])\n",
    "    dynamic_posterior_mean.shape\n",
    "    n_obs = person_data.shape[0]\n",
    "    pred_rt_dynamic = np.zeros(n_obs)\n",
    "\n",
    "    for t in range(n_obs):\n",
    "        # Run diffusion process\n",
    "        pred_rt_dynamic[t] = diffusion_trial(dynamic_posterior_mean[t, context[t]],\n",
    "                                             dynamic_posterior_mean[t, 4],\n",
    "                                             dynamic_posterior_mean[t, 5])\n",
    "\n",
    "    tmp = pd.DataFrame({'id': sub,\n",
    "                        'rt_fast_dm': pred_rt_fast_dm,\n",
    "                        'rt_dynamic': pred_rt_dynamic,\n",
    "                        'rt_empiric': emp_rt})\n",
    "\n",
    "    pred_data = pred_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.to_csv('model_comp_rt_pred.csv',\n",
    "          index=False,\n",
    "          sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_posterior_mean = np.array(post_d.mean()[0])\n",
    "dynamic_posterior_mean.shape\n",
    "n_obs = person_data.shape[0]\n",
    "pred_rt_dynamic = np.zeros(n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = person_data['stim_type'] - 1\n",
    "pred_rt_fast_dm = fast_dm_simulate(params, context)\n",
    "len(pred_rt_fast_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_posterior_mean = np.array(post_d.mean()[0])\n",
    "dynamic_posterior_mean.shape\n",
    "n_obs = person_data.shape[0]\n",
    "pred_rt_dynamic = np.zeros(n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(n_obs):\n",
    "    # Run diffusion process\n",
    "    pred_rt_dynamic[t] = diffusion_trial(dynamic_posterior_mean[t, context[t]], dynamic_posterior_mean[t, 4], dynamic_posterior_mean[t, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(np.abs(pred_rt_fast_dm), fill= '#852626', color=\"#852626\", alpha=0.5, linewidth=0, label=\"Prediction: Fast-dm\")\n",
    "sns.kdeplot(pred_rt_dynamic,fill=\"#598f70\", color=\"#598f70\", alpha=0.5, linewidth=0, label=\"Prediction: Dynamic\")\n",
    "sns.kdeplot(np.abs(person_data.rt.values), fill=\"gray\", color=\"gray\", linewidth=0, alpha=0.3, label=\"Empiric\")\n",
    "sns.despine()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rt_fast_dm.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'rt_fast_dm': pred_rt_fast_dm,\n",
    "        'rt_dynamic': pred_rt_dynamic,\n",
    "        'rt_empiric': person_data.rt.values}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('model_comp_rt_pred.csv',\n",
    "          index=False,\n",
    "          sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5c1a6784774cb00c4ca0a99fd750e2e4caf18aa135f6d49553516d8651234b4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
