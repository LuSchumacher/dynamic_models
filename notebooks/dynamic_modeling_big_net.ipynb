{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from tqdm.notebook import tqdm\n",
    "import talib\n",
    "import pickle\n",
    "import pystan\n",
    "import stan_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# multiprocessing.set_start_method(\"fork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_prior(batch_size):\n",
    "    \"\"\"\n",
    "    Generates a random draw from the diffusion model prior.\n",
    "    \"\"\"\n",
    "    v = np.random.gamma(2.5, 1/1.5, (batch_size, 4))\n",
    "    a = np.random.gamma(4.0, 1/3.0, batch_size)\n",
    "    ndt = np.random.gamma(1.5, 1/5.0, batch_size)\n",
    "    hyper_params = np.random.uniform(0.01, 0.1, (batch_size, 6))\n",
    "\n",
    "    return np.c_[v, a, ndt, hyper_params]\n",
    "\n",
    "\n",
    "@njit\n",
    "def context_gen(batch_size, n_obs):\n",
    "    obs_per_condition = int(n_obs / 4)\n",
    "    context = np.zeros((batch_size, n_obs), dtype=np.int32)\n",
    "    x = np.repeat([1, 2, 3, 4], obs_per_condition)\n",
    "    for i in range(batch_size):\n",
    "        np.random.shuffle(x)\n",
    "        context[i] = x\n",
    "    return context\n",
    "\n",
    "\n",
    "@njit\n",
    "def diffusion_trial(v, a, ndt, zr=0.5, dt=0.001, s=1.0, max_iter=1e4):\n",
    "    \"\"\"\n",
    "    Simulates a single reaction time from a simple drift-diffusion process.\n",
    "    \"\"\"\n",
    "\n",
    "    n_iter = 0\n",
    "    x = a * zr\n",
    "    c = np.sqrt(dt * s)\n",
    "    \n",
    "    while x > 0 and x < a:\n",
    "        \n",
    "        # DDM equation\n",
    "        x += v*dt + c * np.random.randn()\n",
    "        \n",
    "        n_iter += 1\n",
    "        \n",
    "    rt = n_iter * dt\n",
    "    return rt+ndt if x > 0 else -(rt+ndt)\n",
    "\n",
    "\n",
    "@njit\n",
    "def dynamic_diffusion_process(prior_samples, context, n_obs):\n",
    "    \"\"\"\n",
    "    Performs one run of a dynamic diffusion model process.\n",
    "    \"\"\"\n",
    "    \n",
    "    params, params_stds = np.split(prior_samples, 2, axis=-1)\n",
    "    params_t = params\n",
    "    \n",
    "    params_t_array = np.zeros((n_obs, params.shape[0]))\n",
    "    \n",
    "    # Draw first param combination from prior\n",
    "    rt = np.zeros(n_obs)\n",
    "    \n",
    "    # Iterate over number of trials\n",
    "    for t in range(n_obs):\n",
    "        \n",
    "        # Run diffusion process\n",
    "        rt[t] = diffusion_trial(params_t[context[t] - 1], params_t[4], params_t[5])\n",
    "        \n",
    "        # Store before transition\n",
    "        params_t_array[t] = params_t\n",
    "        \n",
    "        # Transition and ensure non-negative parameters\n",
    "        params_t = params_t + params_stds * np.random.randn(params.shape[0])\n",
    "        \n",
    "        # Constraints\n",
    "        params_t[0] = min(max(params_t[0], 0.0), 8)\n",
    "        params_t[1] = min(max(params_t[1], 0.0), 8)\n",
    "        params_t[2] = min(max(params_t[2], 0.0), 8)\n",
    "        params_t[3] = min(max(params_t[3], 0.0), 8)\n",
    "        params_t[4] = min(max(params_t[4], 0.0), 6)\n",
    "        params_t[5] = min(max(params_t[5], 0.0), 4)\n",
    "        \n",
    "    return np.atleast_2d(rt).T, params_t_array, params_stds\n",
    "\n",
    "\n",
    "@njit\n",
    "def batch_simulator(prior_samples, context, n_obs):\n",
    "    \n",
    "    batch_size = prior_samples.shape[0]\n",
    "    rt = np.zeros((batch_size, n_obs, 1))\n",
    "    theta_d = np.zeros((batch_size, n_obs, 6))\n",
    "    theta_s = np.zeros((batch_size, 6))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        rt[i], theta_d[i], theta_s[i] = dynamic_diffusion_process(prior_samples[i], \n",
    "                                                                  context[i],\n",
    "                                                                  n_obs)\n",
    "    \n",
    "    return rt, theta_d, theta_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def static_diffusion_process(prior_samples, context, n_obs):\n",
    "    \"\"\"\n",
    "    Performs one run of a static diffusion model process.\n",
    "    \"\"\"\n",
    "    \n",
    "    params_t, params_stds = np.split(prior_samples, 2, axis=-1)\n",
    "    \n",
    "    rt = np.zeros(n_obs)\n",
    "    \n",
    "    # Iterate over number of trials\n",
    "    for t in range(n_obs):\n",
    "        \n",
    "        # Run diffusion process\n",
    "        rt[t] = diffusion_trial(params_t[context[t] - 1], params_t[4], params_t[5])\n",
    "        \n",
    "    return np.atleast_2d(rt).T, params_t\n",
    "    \n",
    "\n",
    "@njit\n",
    "def static_batch_simulator(prior_samples, context, n_obs):\n",
    "    \n",
    "    batch_size = prior_samples.shape[0]\n",
    "    rt = np.zeros((batch_size, n_obs, 1))\n",
    "    theta = np.zeros((batch_size, n_obs, 6))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rt[i], theta[i] = static_diffusion_process(prior_samples[i], \n",
    "                                                   context[i],\n",
    "                                                   n_obs)\n",
    "    return rt, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stan modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N;                 \n",
    "  real<lower=0> rt[N];    \n",
    "  int<lower=0,upper=1> correct[N];\n",
    "  int<lower=1,upper=4> context[N];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> v[4];\n",
    "  real<lower=0> a; \n",
    "  real<lower=0> ndt;\n",
    "}\n",
    "\n",
    "model {\n",
    "  // Priors\n",
    "  v ~ gamma(2.5, 1.5);\n",
    "  a ~ gamma(4.0, 3.0);\n",
    "  ndt ~ gamma(1.5, 5.0);\n",
    "  \n",
    "  for (n in 1:N) {\n",
    "     if (correct[n] == 1) {\n",
    "        rt[n] ~ wiener(a, ndt, 0.5, v[context[n]]);\n",
    "     } \n",
    "     else {\n",
    "        rt[n] ~ wiener(a, ndt, 1 - 0.5, -v[context[n]]);\n",
    "     }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compile stan model\n",
    "# sm = pystan.StanModel(model_code=stan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_stan(sim_data):\n",
    "    \"\"\"\n",
    "    Convert data from simulator to stan-friendly format.\n",
    "    \"\"\"\n",
    "    rt = sim_data[:, 0]\n",
    "    context = sim_data[:, 1].astype(np.int32)\n",
    "    correct = (rt >= 0).astype(np.int32)\n",
    "    rt = np.abs(rt).astype(np.float32)\n",
    "    return {'rt': rt, 'correct': correct, 'context': context, 'N': rt.shape[0]}\n",
    "\n",
    "def loop_stan(data, verbose=True):\n",
    "    \"\"\"\n",
    "    Loop through data and obtain posteriors.\n",
    "    \"\"\"\n",
    "    \n",
    "    stan_post_samples = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_i = to_stan(data[i])\n",
    "        ndt_init = data_i['rt'].min() * 0.75\n",
    "        init = {'ndt': ndt_init}\n",
    "        fit = sm.sampling(data=data_i, \n",
    "                          iter=2000, chains=4, n_jobs=4, init=[init, init, init, init],\n",
    "                          control=dict(adapt_delta=0.99, max_treedepth=15))\n",
    "        samples = fit.extract(permuted=True)\n",
    "        stan_post_samples.append(samples)\n",
    "        if verbose:\n",
    "            print(f'Finished estimating data set {i + 1}...')\n",
    "    return stan_post_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_OBS = 800\n",
    "# batch_size = 100\n",
    "# prior_draws = dynamic_prior(batch_size)\n",
    "# context = context_gen(batch_size, N_OBS)\n",
    "# sim_data, params_t = static_batch_simulator(prior_draws, context, N_OBS)\n",
    "# stan_post_samples = loop_stan(sim_data)\n",
    "# pickle.dump(stan_post_samples, open('./stan_posteriors_mine.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amortizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroscedasticNetwork(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_params_d):\n",
    "        super(HeteroscedasticNetwork, self).__init__()\n",
    "        \n",
    "        self.preprocessor = Sequential([\n",
    "            Dense(8, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            LSTM(512, return_sequences=True),\n",
    "        ])\n",
    "        \n",
    "        self.dynamic_predictor = Sequential([\n",
    "            Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            tf.keras.layers.Dense(tfpl.MultivariateNormalTriL.params_size(n_params_d)),\n",
    "            tfpl.MultivariateNormalTriL(n_params_d)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain representation\n",
    "        rep = self.preprocessor(x)\n",
    "        \n",
    "        # Predict dynamic stuff\n",
    "        preds_dyn = self.dynamic_predictor(rep)\n",
    "     \n",
    "        return preds_dyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network, optimizer, batch_size, steps_per_epoch, p_bar):\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            # Simulate from model\n",
    "            prior_draws = dynamic_prior(batch_size)\n",
    "            context = context_gen(batch_size, N_OBS)\n",
    "            x_t, params_t, _= batch_simulator(prior_draws, context, N_OBS)\n",
    "            net_in = tf.concat((x_t, context[:, :, np.newaxis]), axis=-1)\n",
    "            pred_params_t = network(net_in)\n",
    "\n",
    "            # Loss dynamic and static\n",
    "            loss_d = nll(params_t, pred_params_t)\n",
    "            \n",
    "            total_loss = loss_d\n",
    "            \n",
    "        g = tape.gradient(total_loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(total_loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss D: {:.3f} Running Loss: {:.3f}\"\n",
    "                              .format(ep, step, loss_d.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS = 800\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "steps_per_epoch = 1000\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "network = HeteroscedasticNetwork(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change learning rate\n",
    "epochs = 20\n",
    "learning_rate = 0.0005\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(1, epochs+1):\n",
    "    with tqdm(total=steps_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "        train_epoch(network, optimizer, batch_size, steps_per_epoch, p_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.save_weights('checkpoints/dynamic_seventh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.load_weights('checkpoints/dynamic_sixth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stan posteriors based on static simulated data\n",
    "pkl_file = open('stan_posteriors_mine.pkl', 'rb')\n",
    "stan_post_samples = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamic_posteriors(dynamic_posterior, par_labels, par_names, \n",
    "                           ground_truths=None, color_pred='#884da3'):\n",
    "    \"\"\"\n",
    "    Inspects the dynamic posterior given a single data set. Assumes six dynamic paramters.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(dynamic_posterior.shape) == 3, \"Dynamic posterior should be 3-dimensional!\" \n",
    "    assert ground_truths is None or len(ground_truths.shape) == 2,'Ground truths should be 2-dimensional!'\n",
    "    assert dynamic_posterior.shape[0] == 1, \"Function assumes dynamics posterior for a single data set!\" \n",
    "        \n",
    "    means = dynamic_posterior.mean()[0]\n",
    "    std = dynamic_posterior.stddev()[0]\n",
    "    \n",
    "    sigma_factors = [1]\n",
    "    alphas = [0.6]\n",
    "    \n",
    "    time = np.arange(x_t.shape[1])\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        \n",
    "        ax.plot(time, means[:, i], color=color_pred, label='Posterior Mean')\n",
    "        for sigma_factor, alpha in zip(sigma_factors, alphas):\n",
    "            ci_upper = means[:, i] + sigma_factor * std[:, i]\n",
    "            ci_lower = means[:, i] - sigma_factor * std[:, i]\n",
    "            ax.fill_between(time, ci_upper, ci_lower, color=color_pred, alpha=alpha)\n",
    "        if ground_truths is not None:\n",
    "            ax.plot(time, ground_truths[:, i], color='black', linestyle='dashed', label='True Dynamic', lw=2)\n",
    "        sns.despine(ax=ax)\n",
    "        ax.grid(alpha=0.15)\n",
    "        ax.set_xlabel('Time (t)')\n",
    "        ax.set_ylabel('Parameter value ({})'.format(par_names[i]), fontsize=12)\n",
    "        ax.set_title(par_labels[i] + ' ({})'.format(par_names[i]), fontsize=12)\n",
    "        \n",
    "        if i == 0:\n",
    "            f.legend()\n",
    "    \n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic simulation\n",
    "n_test = 6\n",
    "prior_draws = dynamic_prior(n_test)\n",
    "context_draws = context_gen(n_test, N_OBS)\n",
    "x_t, params_t, params_s = batch_simulator(prior_draws, context_draws.astype(np.int32), N_OBS)\n",
    "x_in = np.concatenate((x_t, context_draws[:, :, np.newaxis]), axis=-1)\n",
    "x_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static simulation\n",
    "N_OBS = 800\n",
    "batch_size = 100\n",
    "prior_draws = dynamic_prior(batch_size)\n",
    "context = context_gen(batch_size, N_OBS)\n",
    "x_t, params_t = static_batch_simulator(prior_draws, context, N_OBS)\n",
    "x_in = np.concatenate((x_t, context[:, :, np.newaxis]), axis=-1)\n",
    "x_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = 3\n",
    "dynamic_posterior = network(x_in[which:(which+1)])\n",
    "ground_truths = params_t[which]\n",
    "par_labels = ['Drift rate 1', 'Drift rate 2', 'Drift rate 3', 'Drift rate 4', 'Threshold', 'Non-decision time']\n",
    "par_names = [r'$v_1$', r'$v_2$', r'$v_3$', r'$v_3$', r'$a$', r'$\\tau$']\n",
    "plot_dynamic_posteriors(dynamic_posterior, par_labels,  par_names, ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_post_sd = np.zeros((100, 6))\n",
    "stan_sd = np.zeros((100, 6))\n",
    "\n",
    "for j in range(100):\n",
    "    dynamic_posterior = network(x_in[j:(j+1)])\n",
    "    dynamic_post_sd[j] = np.array(dynamic_posterior.stddev()[0, 119])\n",
    "    \n",
    "    v = stan_post_samples[j]['v']\n",
    "    a = stan_post_samples[j]['a']\n",
    "    ndt = stan_post_samples[j]['ndt']\n",
    "\n",
    "    stan_posterior = np.c_[v, a, ndt]\n",
    "    stan_sd[j] = stan_posterior.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_post_sd.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_sd.mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5c1a6784774cb00c4ca0a99fd750e2e4caf18aa135f6d49553516d8651234b4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
