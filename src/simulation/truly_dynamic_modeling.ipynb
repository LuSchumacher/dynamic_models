{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from tqdm.notebook import tqdm\n",
    "# import talib\n",
    "import pickle\n",
    "import pystan\n",
    "\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('../')))\n",
    "from generative_models import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu setting and checking\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"\"\"\n",
    "data {\n",
    "  int<lower=0> N;                 \n",
    "  real<lower=0> rt[N];    \n",
    "  int<lower=0,upper=1> correct[N];\n",
    "  int<lower=1,upper=4> context[N];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> v[4];\n",
    "  real<lower=0> a; \n",
    "  real<lower=0> ndt;\n",
    "}\n",
    "\n",
    "model {\n",
    "  // Priors\n",
    "  v ~ gamma(2.5, 1.5);\n",
    "  a ~ gamma(4.0, 3.0);\n",
    "  ndt ~ gamma(1.5, 5.0);\n",
    "  \n",
    "  for (n in 1:N) {\n",
    "     if (correct[n] == 1) {\n",
    "        rt[n] ~ wiener(a, ndt, 0.5, v[context[n]]);\n",
    "     } \n",
    "     else {\n",
    "        rt[n] ~ wiener(a, ndt, 1 - 0.5, -v[context[n]]);\n",
    "     }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_stan(sim_data):\n",
    "    \"\"\"\n",
    "    Convert data from simulator to stan-friendly format.\n",
    "    \"\"\"\n",
    "    rt = sim_data[:, 0]\n",
    "    context = sim_data[:, 1].astype(np.int32) + 1\n",
    "    correct = (rt >= 0).astype(np.int32)\n",
    "    rt = np.abs(rt).astype(np.float32)\n",
    "    return {'rt': rt, 'correct': correct, 'context': context, 'N': rt.shape[0]}\n",
    "\n",
    "def loop_stan(data, verbose=True):\n",
    "    \"\"\"\n",
    "    Loop through data and obtain posteriors.\n",
    "    \"\"\"\n",
    "    \n",
    "    stan_post_samples = []\n",
    "    for i in range(data.shape[0]):\n",
    "        data_i = to_stan(data[i])\n",
    "        ndt_init = data_i['rt'].min() * 0.75\n",
    "        init = {'ndt': ndt_init}\n",
    "        fit = sm.sampling(data=data_i, \n",
    "                          iter=2000, chains=4, n_jobs=4, init=[init, init, init, init],\n",
    "                          control=dict(adapt_delta=0.99, max_treedepth=15))\n",
    "        samples = fit.extract(permuted=True)\n",
    "        stan_post_samples.append(samples)\n",
    "        if verbose:\n",
    "            print(f'Finished estimating data set {i + 1}...')\n",
    "    return stan_post_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS = 3200\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelization for stan\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"fork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile stan model\n",
    "sm = pystan.StanModel(model_code=stan_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No variance (static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_draws = dynamic_prior(BATCH_SIZE)\n",
    "context = context_gen(BATCH_SIZE, N_OBS)\n",
    "sim_data, theta_d = static_batch_simulator(prior_draws, context)\n",
    "sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "which_param = 0\n",
    "plt.plot(np.arange(N_OBS), theta_d[0, :, which_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_post_samples = loop_stan(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(\n",
    "# {\n",
    "#     'rt': sim_data[:, : , 0], \n",
    "#     'context': sim_data[:, : , 1], \n",
    "#     'theta_d': theta_d,\n",
    "#     'stan_post_samples': stan_post_samples\n",
    "#     }, \n",
    "#     open('./simulation_800_noVar.pkl', 'wb+')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_draws = dynamic_prior(BATCH_SIZE, fixed_var=0.001)\n",
    "context = context_gen(BATCH_SIZE, N_OBS)\n",
    "sim_data, theta_d, theta_s = dynamic_batch_simulator(prior_draws, context)\n",
    "sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "which_param = 0\n",
    "plt.plot(np.arange(N_OBS), theta_d[0, :, which_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_post_samples = loop_stan(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(\n",
    "# {\n",
    "#     'rt': sim_data[:, : , 0], \n",
    "#     'context': sim_data[:, : , 1], \n",
    "#     'theta_d': theta_d,\n",
    "#     'theta_s': theta_s,\n",
    "#     'stan_post_samples': stan_post_samples\n",
    "#     }, \n",
    "#     open('./sim_800_lowVar.pkl', 'wb+')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_draws = dynamic_prior(BATCH_SIZE, fixed_var=0.1)\n",
    "context = context_gen(BATCH_SIZE, N_OBS)\n",
    "sim_data, theta_d, theta_s = dynamic_batch_simulator(prior_draws, context)\n",
    "sim_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "which_param = 0\n",
    "plt.plot(np.arange(N_OBS), theta_d[0, :, which_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_post_samples = loop_stan(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(\n",
    "# {\n",
    "#     'rt': sim_data[:, : , 0], \n",
    "#     'context': sim_data[:, : , 1], \n",
    "#     'theta_d': theta_d,\n",
    "#     'theta_s': theta_s,\n",
    "#     'stan_post_samples': stan_post_samples\n",
    "#     }, \n",
    "#     open('./sim_800_highVar.pkl', 'wb+')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amortizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HeteroscedasticNetwork(tf.keras.Model):\n",
    "    \n",
    "#     def __init__(self, n_params_d):\n",
    "#         super(HeteroscedasticNetwork, self).__init__()\n",
    "        \n",
    "#         self.preprocessor = Sequential([\n",
    "#             GRU(64, return_sequences=True),\n",
    "#             LSTM(128, return_sequences=True),\n",
    "#             Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "#         ])\n",
    "        \n",
    "#         self.dynamic_predictor = Sequential([\n",
    "#             Dense(64, activation='selu', kernel_initializer='lecun_normal'),\n",
    "#             tf.keras.layers.Dense(tfpl.MultivariateNormalTriL.params_size(n_params_d)),\n",
    "#             tfpl.MultivariateNormalTriL(n_params_d)\n",
    "#         ])\n",
    "        \n",
    "#     def call(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass through the model.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Obtain representation\n",
    "#         rep = self.preprocessor(x)\n",
    "        \n",
    "#         # Predict dynamic stuff\n",
    "#         preds_dyn = self.dynamic_predictor(rep)\n",
    "     \n",
    "#         return preds_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEEEEEEWWW\n",
    "class HeteroscedasticNetwork(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_params_d, n_params_s):\n",
    "        super(HeteroscedasticNetwork, self).__init__()\n",
    "        \n",
    "        self.preprocessor = Sequential([\n",
    "            GRU(64, return_sequences=True),\n",
    "            LSTM(128, return_sequences=True),\n",
    "            Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "        ])\n",
    "        \n",
    "        self.dynamic_predictor = Sequential([\n",
    "            Dense(64, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            tf.keras.layers.Dense(tfpl.MultivariateNormalTriL.params_size(n_params_d)),\n",
    "            tfpl.MultivariateNormalTriL(n_params_d)\n",
    "        ])\n",
    "\n",
    "        self.static_predictor = Sequential([\n",
    "            LSTM(n_params_s),\n",
    "            Dense(tfpl.MultivariateNormalTriL.params_size(n_params_s)),\n",
    "            tfpl.MultivariateNormalTriL(n_params_s)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain representation\n",
    "        rep = self.preprocessor(x)\n",
    "        \n",
    "        # Predict dynamic\n",
    "        preds_dyn = self.dynamic_predictor(rep)\n",
    "\n",
    "        # predict static\n",
    "        preds_stat = self.static_predictor(rep)\n",
    "\n",
    "        return preds_dyn, preds_stat\n",
    "\n",
    "class StaticHeteroscedasticNetwork(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_params_d):\n",
    "        super(StaticHeteroscedasticNetwork, self).__init__()\n",
    "        \n",
    "        self.preprocessor = Sequential([\n",
    "            GRU(64, return_sequences=True),\n",
    "            LSTM(128, return_sequences=True),\n",
    "            Dense(128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "        ])\n",
    "        \n",
    "        self.dynamic_predictor = Sequential([\n",
    "            Dense(64, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            tf.keras.layers.Dense(tfpl.MultivariateNormalTriL.params_size(n_params_d)),\n",
    "            tfpl.MultivariateNormalTriL(n_params_d)\n",
    "        ])\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \"\"\"\n",
    "        # Obtain representation\n",
    "        rep = self.preprocessor(x)\n",
    "        \n",
    "        # Predict dynamic\n",
    "        preds_dyn = self.dynamic_predictor(rep)\n",
    "\n",
    "        return preds_dyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def static_epoch_trainer(network, optimizer, batch_size, n_obs, steps_per_epoch, p_bar):\n",
    "    \n",
    "#     losses = []\n",
    "#     for step in range(1, steps_per_epoch+1):\n",
    "#         with tf.GradientTape() as tape:\n",
    "        \n",
    "#             # Simulate from model\n",
    "#             prior_draws = dynamic_prior(batch_size)\n",
    "#             context = context_gen(batch_size, n_obs)\n",
    "#             sim_data, theta_d = static_batch_simulator(prior_draws, context)\n",
    "\n",
    "#             net_in = tf.concat((sim_data[:, :, :1], to_categorical(sim_data[:, :, 1:])), axis=-1)\n",
    "#             pred_theta_d = network(net_in)\n",
    "\n",
    "#             # Loss \n",
    "#             loss_d = nll(theta_d, pred_theta_d)\n",
    "            \n",
    "#             total_loss = loss_d\n",
    "#         g = tape.gradient(total_loss, network.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "#         losses.append(total_loss.numpy())\n",
    "\n",
    "#         # Update progress bar\n",
    "#         p_bar.set_postfix_str(\"Ep: {},Step {},Loss D: {:.3f} Running Loss: {:.3f}\"\n",
    "#                               .format(ep, step, loss_d.numpy(), np.mean(losses)))\n",
    "#         p_bar.update(1)\n",
    "\n",
    "\n",
    "# def dynamic_epoch_trainer(network, optimizer, batch_size, n_obs, steps_per_epoch, p_bar, fixed_var):\n",
    "#     losses = []\n",
    "#     for step in range(1, steps_per_epoch+1):\n",
    "#         with tf.GradientTape() as tape:\n",
    "        \n",
    "#             # Simulate from model\n",
    "#             prior_draws = dynamic_prior(batch_size, fixed_var)\n",
    "#             context = context_gen(batch_size, n_obs)\n",
    "#             sim_data, theta_d, _= dynamic_batch_simulator(prior_draws, context)\n",
    "\n",
    "#             net_in = tf.concat((sim_data[:, :, :1], to_categorical(sim_data[:, :, 1:])), axis=-1)\n",
    "#             pred_theta_d = network(net_in)\n",
    "\n",
    "#             # Loss dynamic and static\n",
    "#             loss_d = nll(theta_d, pred_theta_d)\n",
    "            \n",
    "#             total_loss = loss_d\n",
    "#         g = tape.gradient(total_loss, network.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "#         losses.append(total_loss.numpy())\n",
    "\n",
    "#         # Update progress bar\n",
    "#         p_bar.set_postfix_str(\"Ep: {},Step {},Loss D: {:.3f} Running Loss: {:.3f}\"\n",
    "#                               .format(ep, step, loss_d.numpy(), np.mean(losses)))\n",
    "#         p_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEEEWW!\n",
    "def epoch_trainer(network, optimizer, batch_size, n_obs, steps_per_epoch, p_bar, fixed_var):\n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Simulate from model\n",
    "            prior_draws = dynamic_prior(batch_size, fixed_var)\n",
    "            context = context_gen(batch_size, n_obs)\n",
    "            sim_data, theta_d, theta_s = dynamic_batch_simulator(prior_draws, context)\n",
    "\n",
    "            # predict\n",
    "            net_in = tf.concat((sim_data[:, :, :1], to_categorical(sim_data[:, :, 1:])), axis=-1)\n",
    "            pred_theta_d, pred_theta_s = network(net_in)\n",
    "\n",
    "            # loss\n",
    "            loss_d = nll(theta_d, pred_theta_d)\n",
    "            loss_s = nll(theta_s, pred_theta_s)\n",
    "            total_loss = loss_d + loss_s\n",
    "\n",
    "        g = tape.gradient(total_loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(total_loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss D: {:.3f} Running Loss: {:.3f}\"\n",
    "                              .format(ep, step, loss_d.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n",
    "\n",
    "def static_epoch_trainer(network, optimizer, batch_size, n_obs, steps_per_epoch, p_bar):\n",
    "    \n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            # Simulate from model\n",
    "            prior_draws = dynamic_prior(batch_size)\n",
    "            context = context_gen(batch_size, n_obs)\n",
    "            sim_data, theta_d = static_batch_simulator(prior_draws, context)\n",
    "\n",
    "            net_in = tf.concat((sim_data[:, :, :1], to_categorical(sim_data[:, :, 1:])), axis=-1)\n",
    "            pred_theta_d = network(net_in)\n",
    "\n",
    "            # Loss \n",
    "            loss_d = nll(theta_d, pred_theta_d)\n",
    "            \n",
    "            total_loss = loss_d\n",
    "        g = tape.gradient(total_loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(total_loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss D: {:.3f} Running Loss: {:.3f}\"\n",
    "                              .format(ep, step, loss_d.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS = 3200\n",
    "BATCH_SIZE = 8\n",
    "STEPS_PER_EPOCH = 1000\n",
    "epochs = [30, 30, 20]\n",
    "learning_rates = [0.0001, 0.00005, 0.00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No variance (static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = StaticHeteroscedasticNetwork(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(epochs)):\n",
    "    # set learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rates[i])\n",
    "    \n",
    "    # train epochs\n",
    "    for ep in range(1, epochs[i]+1):\n",
    "        with tqdm(total=STEPS_PER_EPOCH, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "            static_epoch_trainer(network, optimizer, BATCH_SIZE, N_OBS, STEPS_PER_EPOCH, p_bar)\n",
    "    \n",
    "    #save weights\n",
    "    network.save_weights('checkpoints/ablation_static_3200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = HeteroscedasticNetwork(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(epochs)):\n",
    "    # set learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rates[i])\n",
    "    \n",
    "    # train epochs\n",
    "    for ep in range(1, epochs[i]+1):\n",
    "        with tqdm(total=STEPS_PER_EPOCH, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "            epoch_trainer(network, optimizer, BATCH_SIZE, N_OBS, STEPS_PER_EPOCH, p_bar, 0.001)\n",
    "    \n",
    "    #save weights\n",
    "    network.save_weights('checkpoints/ablation_lowVar_3200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = HeteroscedasticNetwork(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(epochs)):\n",
    "    # set learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rates[i])\n",
    "    \n",
    "    # train epochs\n",
    "    for ep in range(1, epochs[i]+1):\n",
    "        with tqdm(total=STEPS_PER_EPOCH, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "            epoch_trainer(network, optimizer, BATCH_SIZE, N_OBS, STEPS_PER_EPOCH, p_bar, 0.1)\n",
    "    \n",
    "    #save weights\n",
    "    network.save_weights('checkpoints/ablation_highVar_3200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained network\n",
    "network_no_var   = HeteroscedasticNetwork(6)\n",
    "network_low_var  = HeteroscedasticNetwork(6)\n",
    "network_high_var = HeteroscedasticNetwork(6)\n",
    "network_no_var.load_weights('checkpoints/static_800')\n",
    "network_low_var.load_weights('checkpoints/lowVar_800')\n",
    "network_high_var.load_weights('checkpoints/highVar_800')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_post_sd(stan_post_samples, bf_post_no_var, bf_post_low_var, bf_post_high_var, res, types):\n",
    "    # get mean and sd of posterior sd's from Bayesflow fits\n",
    "    bf_post_no_var_sd   = np.array(bf_post_no_var.stddev()).mean(axis=0)\n",
    "    bf_post_low_var_sd  = np.array(bf_post_low_var.stddev()).mean(axis=0)\n",
    "    bf_post_high_var_sd = np.array(bf_post_high_var.stddev()).mean(axis=0)\n",
    "    bf_post_no_var_sd_sd   = np.array(bf_post_no_var.stddev()).std(axis=0)\n",
    "    bf_post_low_var_sd_sd  = np.array(bf_post_low_var.stddev()).std(axis=0)\n",
    "    bf_post_high_var_sd_sd = np.array(bf_post_high_var.stddev()).std(axis=0)\n",
    "\n",
    "    # get mean and sd posterior sd's from Stan fits\n",
    "    stan_sd = np.zeros((100, 6))\n",
    "    for j in range(100):\n",
    "        v = stan_post_samples[j]['v']\n",
    "        a = stan_post_samples[j]['a']\n",
    "        ndt = stan_post_samples[j]['ndt']\n",
    "\n",
    "        stan_posterior = np.c_[v, a, ndt]\n",
    "        stan_sd[j] = stan_posterior.std(axis=0)\n",
    "\n",
    "    stan_sd_sd = stan_sd.std(axis=0)\n",
    "    stan_sd = stan_sd.mean(axis=0)\n",
    "\n",
    "    # plotting info\n",
    "    labels = ['No variance', 'Low variance', 'High variance', \"Static Stan\"]\n",
    "    param_labels = ['Drift rate 1', 'Drift rate 2', 'Drift rate 3', 'Drift rate 4', 'Threshold', 'Non-decision time']\n",
    "    colors = ['#D18186', '#7697CA', '#4f8752', \"#000000\"]\n",
    "    param_names = [r'$v_1$', r'$v_2$', r'$v_3$', r'$v_4$', r'$a$', r'$\\tau$']\n",
    "    alpha = 0.8\n",
    "    linewidth = 3\n",
    "\n",
    "    # plotting time window\n",
    "    x_min = int(bf_post_no_var_sd.shape[0] - res)\n",
    "    time = np.arange(x_min, bf_post_no_var_sd.shape[0])\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8), constrained_layout=True)\n",
    "\n",
    "    # iterate over parameters\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        # plot Bayesflow results\n",
    "        ax.plot(time, bf_post_no_var_sd[x_min:, i], color=colors[0], label=labels[0], alpha=alpha, linewidth=linewidth)\n",
    "        ax.plot(time, bf_post_low_var_sd[x_min:, i], color=colors[1], label=labels[1], alpha=alpha, linewidth=linewidth)\n",
    "        ax.plot(time, bf_post_high_var_sd[x_min:, i], color=colors[2], label=labels[2], alpha=alpha, linewidth=linewidth)\n",
    "        ax.fill_between(time, bf_post_no_var_sd[x_min:, i]+bf_post_no_var_sd_sd[x_min:, i]/2, bf_post_no_var_sd[x_min:, i]-bf_post_no_var_sd_sd[x_min:, i]/2, color=colors[0], alpha=0.2)\n",
    "        ax.fill_between(time, bf_post_low_var_sd[x_min:, i]+bf_post_low_var_sd_sd[x_min:, i]/2, bf_post_low_var_sd[x_min:, i]-bf_post_low_var_sd_sd[x_min:, i]/2, color=colors[1], alpha=0.2)\n",
    "        ax.fill_between(time, bf_post_high_var_sd[x_min:, i]+bf_post_high_var_sd_sd[x_min:, i]/2, bf_post_high_var_sd[x_min:, i]-bf_post_high_var_sd_sd[x_min:, i]/2, color=colors[2], alpha=0.2)\n",
    "\n",
    "        # plot Stan results\n",
    "        ax.hlines(y=stan_sd[i], xmin=x_min, xmax=800, color=colors[3], label=labels[3], linestyles=\"dashed\", linewidth=1.5)\n",
    "        stan_sd_array = np.repeat(stan_sd[i], len(time))\n",
    "        ax.fill_between(time, stan_sd_array+stan_sd_sd[i]/2, stan_sd_array-stan_sd_sd[i]/2, color=colors[3], alpha=0.2)\n",
    "\n",
    "        # set appropriate x-axis limits\n",
    "        max_val = np.array([bf_post_no_var_sd[x_min:, i], \n",
    "                            bf_post_low_var_sd[x_min:, i],\n",
    "                            bf_post_high_var_sd[x_min:, i]]).max()\n",
    "        ax.set_ylim(0, max_val + max_val/2)\n",
    "\n",
    "        sns.despine(ax=ax)\n",
    "        ax.set_xlabel('Time (t)')\n",
    "        ax.set_ylabel('Posterior standard deviation', fontsize=12)\n",
    "        ax.set_title(param_labels[i] + ' ({})'.format(param_names[i]), fontsize=12)\n",
    "\n",
    "        if i == 0:  \n",
    "            ax.legend()\n",
    "            f.suptitle(\"Parameter std's over time: {}\".format(labels[types]) + \" simulated data\", fontsize=16)\n",
    "\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_post_mean_error(stan_post_samples, bf_post_no_var, bf_post_low_var, bf_post_high_var, types):\n",
    "    # get mean and sd of posterior means's from Bayesflow fits\n",
    "    bf_post_mean_no_var      = np.array(bf_post_no_var.mean()).mean(axis=0)\n",
    "    bf_post_mean_low_var     = np.array(bf_post_low_var.mean()).mean(axis=0)\n",
    "    bf_post_mean_high_var    = np.array(bf_post_high_var.mean()).mean(axis=0)\n",
    "    bf_post_mean_no_var_sd   = np.array(bf_post_no_var.mean()).std(axis=0)\n",
    "    bf_post_mean_low_var_sd  = np.array(bf_post_low_var.mean()).std(axis=0)\n",
    "    bf_post_mean_high_var_sd = np.array(bf_post_high_var.mean()).std(axis=0)\n",
    "\n",
    "    # get mean and sd posterior mean's from Stan fits\n",
    "    stan_mean = np.zeros((100, 6))\n",
    "    for j in range(100):\n",
    "        v = stan_post_samples[j]['v']\n",
    "        a = stan_post_samples[j]['a']\n",
    "        ndt = stan_post_samples[j]['ndt']\n",
    "\n",
    "        stan_posterior = np.c_[v, a, ndt]\n",
    "        stan_mean[j] = stan_posterior.mean(axis=0)\n",
    "\n",
    "    stan_mean_sd = stan_mean.std(axis=0)\n",
    "    stan_mean = stan_mean.mean(axis=0)\n",
    "\n",
    "    sqrt_err_no_var = (bf_post_mean_no_var - stan_mean)**2\n",
    "    sqrt_err_low_var = (bf_post_mean_low_var - stan_mean)**2\n",
    "    sqrt_err_high_var = (bf_post_mean_high_var - stan_mean)**2\n",
    "\n",
    "    # plotting info\n",
    "    labels = ['No variance', 'Low variance', 'High variance']\n",
    "    param_labels = ['Drift rate 1', 'Drift rate 2', 'Drift rate 3', 'Drift rate 4', 'Threshold', 'Non-decision time']\n",
    "    colors = ['#D18186', '#7697CA', '#4f8752', \"#000000\"]\n",
    "    param_names = [r'$v_1$', r'$v_2$', r'$v_3$', r'$v_4$', r'$a$', r'$\\tau$']\n",
    "    alpha = 0.8\n",
    "    linewidth = 3\n",
    "\n",
    "    # plotting time window\n",
    "    time = np.arange(sqrt_err_no_var.shape[0])\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8), constrained_layout=True)\n",
    "\n",
    "    # iterate over parameters\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        # plot Bayesflow results\n",
    "        ax.plot(time, sqrt_err_no_var[:, i], color=colors[0], label=labels[0], alpha=alpha, linewidth=linewidth)\n",
    "        ax.plot(time, sqrt_err_low_var[:, i], color=colors[1], label=labels[1], alpha=alpha, linewidth=linewidth)\n",
    "        ax.plot(time, sqrt_err_high_var[:, i], color=colors[2], label=labels[2], alpha=alpha, linewidth=linewidth)\n",
    "\n",
    "        # set appropriate x-axis limits\n",
    "        max_val = np.array([sqrt_err_no_var[:, i], \n",
    "                            sqrt_err_low_var[:, i],\n",
    "                            sqrt_err_high_var[:, i]]).max()\n",
    "        ax.set_ylim(0, max_val + max_val/2)\n",
    "\n",
    "        sns.despine(ax=ax)\n",
    "        ax.set_xlabel('Time (t)')\n",
    "        ax.set_ylabel('Squared Error of posterior mean', fontsize=12)\n",
    "        ax.set_title(param_labels[i] + ' ({})'.format(param_names[i]), fontsize=12)\n",
    "\n",
    "        if i == 0:  \n",
    "            ax.legend()\n",
    "            f.suptitle(\"Difference between posterior means of BayesFlow and Stan: 100 {}\".format(labels[types]) + \" simulated data\", fontsize=16)\n",
    "\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No variance (static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load simulated data and stan fits\n",
    "sim = pd.read_pickle('./sim_800_noVar.pkl')\n",
    "rt = sim['rt']\n",
    "context = to_categorical(sim['context'])\n",
    "sim_data = np.concatenate((rt[:, :, np.newaxis], context), axis=-1)\n",
    "stan_post_samples = sim['stan_post_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amortized inference\n",
    "bf_post_no_var   = network_no_var(sim_data)\n",
    "bf_post_low_var  = network_low_var(sim_data)\n",
    "bf_post_high_var = network_high_var(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_post_sd(stan_post_samples, bf_post_no_var, bf_post_low_var, bf_post_high_var, 800, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_post_mean_error(stan_post_samples, bf_post_no_var, bf_post_low_var, bf_post_high_var, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load simulated data and stan fits\n",
    "sim = pd.read_pickle('./sim_800_lowVar.pkl')\n",
    "rt = sim['rt']\n",
    "context = to_categorical(sim['context'])\n",
    "sim_data = np.concatenate((rt[:, :, np.newaxis], context), axis=-1)\n",
    "true_theta_d = sim['theta_d']\n",
    "stan_post_samples = sim['stan_post_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amortized inference\n",
    "bf_post_no_var   = network_no_var(sim_data)\n",
    "bf_post_low_var  = network_low_var(sim_data)\n",
    "bf_post_high_var = network_high_var(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_post_sd(stan_post_samples, bf_post_no_var, bf_post_low_var, bf_post_high_var, 800, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_post_mean_error(stan_post_samples, bf_post_no_var, bf_post_low_var, bf_post_high_var, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load simulated data and stan fits\n",
    "sim = pd.read_pickle('./sim_800_highVar.pkl')\n",
    "rt = sim['rt']\n",
    "context = to_categorical(sim['context'])\n",
    "sim_data = np.concatenate((rt[:, :, np.newaxis], context), axis=-1)\n",
    "true_theta_d = sim['theta_d']\n",
    "stan_post_samples = sim['stan_post_samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amortized inference\n",
    "bf_post_no_var   = network_no_var(sim_data)\n",
    "bf_post_low_var  = network_low_var(sim_data)\n",
    "bf_post_high_var = network_high_var(sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_post_sd(stan_post_samples, bf_post_no_var, bf_post_low_var, bf_post_high_var, 800, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_post_mean_error(stan_post_samples, bf_post_no_var, bf_post_low_var, bf_post_high_var, 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5c1a6784774cb00c4ca0a99fd750e2e4caf18aa135f6d49553516d8651234b4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
